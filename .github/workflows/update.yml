name: Update Data

on:
  schedule:
    - cron: '0 4 * * *' # Täglich 4 Uhr morgens
  workflow_dispatch:    # Manuell startbar unter "Actions"

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Download Data
        run: |
          # 1. Wikidata
          curl -G "https://query.wikidata.org/sparql" \
            --data-urlencode "query=SELECT DISTINCT ?qid ?lat ?lon ?label WHERE { ?item wdt:P131* wd:Q15124; wdt:P625 ?loc . BIND(STRAFTER(STR(?item), '/entity/') as ?qid) BIND(geof:latitude(?loc) as ?lat) BIND(geof:longitude(?loc) as ?lon) OPTIONAL { ?item rdfs:label ?label. FILTER(lang(?label)='de') } }" \
            -H "Accept: text/csv" > query.csv
          
          # 2. OSM (Robustere Query mit Timeout)
          # Wir nutzen die ID 3600016240 für Südtirol und suchen Keys mit 'wikidata' drin
          curl -d "data=[out:json][timeout:600];area(id:3600016240)->.searchArea;nwr[~'wikidata'~'.'](area.searchArea);out tags;" \
            "https://overpass-api.de/api/interpreter" > osm.json

      - name: Run Processing
        run: python abgleich.py

      - name: Commit & Push
        run: |
          git config --global user.name 'UpdateBot'
          git config --global user.email 'bot@noreply.github.com'
          git add data.geojson
          git commit -m "Auto Update" || exit 0
          git pull --rebase -Xtheirs origin main
          git push
